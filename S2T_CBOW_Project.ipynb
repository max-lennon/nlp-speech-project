{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uu4FmfhAo8_O",
    "outputId": "078abd3f-21d5-4576-8dfa-584c6a9d84b9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/max/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from os.path import isfile, isdir, join\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import time\n",
    "import random\n",
    "\n",
    "import nltk, string, re\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "\n",
    "# RegEx to capture punctuation and numbers\n",
    "punct = re.compile(f'^[{re.escape(string.punctuation)}]+$')\n",
    "num = re.compile(\"^[\\d.]+$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LOdOvrKPXuS-",
    "outputId": "f2e9db1e-68a3-434b-9543-03b32e33a9d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-datamuse in /home/max/anaconda3/envs/NLP/lib/python3.9/site-packages (1.3.0)\r\n",
      "Requirement already satisfied: requests in /home/max/anaconda3/envs/NLP/lib/python3.9/site-packages (from python-datamuse) (2.26.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/max/anaconda3/envs/NLP/lib/python3.9/site-packages (from requests->python-datamuse) (2021.10.8)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/max/anaconda3/envs/NLP/lib/python3.9/site-packages (from requests->python-datamuse) (3.3)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/max/anaconda3/envs/NLP/lib/python3.9/site-packages (from requests->python-datamuse) (1.26.7)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/max/anaconda3/envs/NLP/lib/python3.9/site-packages (from requests->python-datamuse) (2.0.7)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install python-datamuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tvJ2ViCQaJ6t",
    "outputId": "e27dccdb-c6b8-4462-982b-30aa5a948566"
   },
   "outputs": [],
   "source": [
    "from datamuse import datamuse\n",
    "import re\n",
    "\n",
    "if not isfile(\"word_replacements.csv\"):\n",
    "  word_replacements = {}\n",
    "else:\n",
    "  word_replacements = pd.read_csv(\"word_replacements.csv\", header=None, index_col=0, squeeze=True).to_dict()\n",
    "\n",
    "api = datamuse.Datamuse()\n",
    "\n",
    "def get_similar_words(word, n=25):\n",
    "\n",
    "  if word not in word_replacements.keys():\n",
    "    query_result = api.words(sl=word)\n",
    "\n",
    "    score = 100\n",
    "    sim_words = []\n",
    "    i = 0\n",
    "    while (score > 90 and len(sim_words) < n):\n",
    "      try:\n",
    "          word = query_result[i]['word']\n",
    "      except IndexError:\n",
    "          break;\n",
    "      word = re.sub('[^A-Za-z0-9]+', '', word)\n",
    "      sim_words.append(word)\n",
    "      score = query_result[i]['score']\n",
    "      i += 1\n",
    "    word_replacements[word] = list(set(sim_words))\n",
    "  return word_replacements[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(itertools.chain(*trn_texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_target_pairs(data, window_size, total_vocab):\n",
    "    tot_length = window_size*2\n",
    "    for seq in data:\n",
    "        text_len = len(seq)\n",
    "        for idx, word in enumerate(seq):\n",
    "            context_word = []\n",
    "            target   = []            \n",
    "            begin = idx - window_size\n",
    "            end = idx + window_size + 1\n",
    "            context_word.append([seq[i] for i in range(begin, end) if 0 <= i < text_len and i != idx])\n",
    "            target.append(word)\n",
    "            contextual = sequence.pad_sequences(context_word, total_length=tot_length)\n",
    "            target = np_utils.to_categorical(target, total_vocab)\n",
    "            yield(contextual, target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn, torch.utils, torch.optim\n",
    "import torchtext\n",
    "from torchtext.datasets import imdb\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-61e398a81f070b0e\n",
      "Reusing dataset text (/home/max/.cache/huggingface/datasets/text/default-61e398a81f070b0e/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a53dfaf4c331422c889514e0bf81c135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(WordLevel(unk_token=\"UNK\"))\n",
    "tokenizer.add_special_tokens(['<pad>', 'UNK'])\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = WordLevelTrainer()\n",
    "tokenizer.train(['arxiv/trn-arxiv.txt'], trainer)\n",
    "tokenizer.enable_padding(pad_id=tokenizer.token_to_id('<pad>'), pad_token='<pad>')\n",
    "\n",
    "dataset = load_dataset('text', data_files={'train': 'arxiv/trn-arxiv.txt', 'dev': 'arxiv/dev-arxiv.txt'})\n",
    "\n",
    "def collate(examples):\n",
    "    texts = [ex['text'] for ex in examples[:1000]]\n",
    "    return torch.LongTensor([text.ids for text in tokenizer.encode_batch(texts)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cbow_classifier(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, input_dim=16, hidden_dim=64, dropout=0):\n",
    "        super().__init__()               \n",
    "        self.embedding = torch.nn.Embedding(vocab_size, input_dim)\n",
    "        self.hidden_layer = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.top_layer = torch.nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, texts):\n",
    "        embedded = self.embedding(texts)\n",
    "        cbow = embedded.mean(dim=0)\n",
    "        cbow_drop = self.dropout(cbow)\n",
    "        hidden = torch.relu(self.hidden_layer(cbow_drop))\n",
    "        scores = self.top_layer(hidden)\n",
    "        return scores \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_validation(scores, loss_function, truth):\n",
    "    guesses = scores.argmax(dim=1)\n",
    "    n_correct = (guesses == truth).sum().item()\n",
    "    for i in range(int(guesses.cpu().numpy().shape[0])):\n",
    "        scores[i][guesses[i]] = 0\n",
    "    guesses_2 = scores.argmax(dim=1)\n",
    "    n_correct_2 = n_correct + (guesses == truth).sum().item()\n",
    "    \n",
    "    return n_correct, n_correct_2, loss_function(scores, truth).item()\n",
    "\n",
    "\n",
    "def eval_with_sound(scores, sounds_like, truth):\n",
    "    max_guesses = []\n",
    "    max_guesses_2 = []\n",
    "    for i, score_vec in enumerate(scores):\n",
    "        sound_like_vec = sounds_like[i]\n",
    "        sound_scores = score_vec[sound_like_vec.ids]\n",
    "        if len(sound_scores) == 0:\n",
    "            max_guesses.append(0)\n",
    "            max_guesses_2.append(0)\n",
    "            continue\n",
    "        max_guesses.append(sound_like_vec.ids[sound_scores.argmax(dim=0)])\n",
    "        if len(sound_scores) == 1:\n",
    "            max_guesses_2.append(0)\n",
    "            continue\n",
    "        sound_scores[sound_scores.argmax(dim=0)] = 0\n",
    "        max_guesses_2.append(sound_like_vec.ids[sound_scores.argmax(dim=0)])\n",
    "    n_correct = (torch.LongTensor(max_guesses).cuda() == truth).sum().item()\n",
    "    n_correct_2 = n_correct + (torch.LongTensor(max_guesses_2).cuda() == truth).sum().item()\n",
    "    return n_correct, n_correct_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = cbow_classifier(vocab_size=len(vocab))   \n",
    "model.train()\n",
    "\n",
    "model.to('cuda')\n",
    "\n",
    "train_iter = torch.utils.data.DataLoader(dataset[\"train\"], batch_size=128, collate_fn=collate)\n",
    "dev_iter = torch.utils.data.DataLoader(dataset[\"dev\"], batch_size=64, collate_fn=collate)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "history = defaultdict(list)\n",
    "\n",
    "def train_model(model, epochs, reset=False):\n",
    "    \n",
    "    if reset:\n",
    "        model = cbow_classifier(vocab_size=len(vocab))   \n",
    "        model.train()\n",
    "\n",
    "        model.to('cuda')\n",
    "\n",
    "    for i in range(epochs):\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        loss_sum = 0\n",
    "        n_batches = 0\n",
    "        model.train()\n",
    "\n",
    "        for b, batch in enumerate(train_iter):\n",
    "\n",
    "            label_idxs = [random.randint(0,11) for seq in batch]\n",
    "            labels = torch.cat(tuple([batch[i][label_idxs[i]:label_idxs[i]+1] for i in range(len(label_idxs))]))\n",
    "            for j in range(len(label_idxs)):\n",
    "                batch[j] = torch.cat((batch[j][:label_idxs[j]], batch[j][label_idxs[j]+1:], batch[j][label_idxs[j]:label_idxs[j]+1]))\n",
    "\n",
    "            batch = batch.transpose(1,0).to('cuda')\n",
    "\n",
    "            scores = model(batch.cuda())\n",
    "\n",
    "            loss = loss_function(scores, labels.cuda())\n",
    "\n",
    "            optimizer.zero_grad()            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_sum += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        train_loss = loss_sum / n_batches\n",
    "        history['train_loss'].append(train_loss)\n",
    "\n",
    "        n_correct = 0\n",
    "        n_valid = len(dev_iter) * 64\n",
    "        loss_sum = 0\n",
    "        n_batches = 0\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        for b, batch in enumerate(dev_iter):\n",
    "            label_idxs = [random.randint(0,11) for seq in batch]\n",
    "            labels = torch.cat(tuple([batch[i][label_idxs[i]:label_idxs[i]+1] for i in range(len(label_idxs))]))\n",
    "            for j in range(len(label_idxs)):\n",
    "                batch[j] = torch.cat((batch[j][:label_idxs[j]], batch[j][label_idxs[j]+1:], batch[j][label_idxs[j]:label_idxs[j]+1]))\n",
    "            scores = model(batch.transpose(1,0).cuda())\n",
    "            n_corr_batch, _, loss_batch = evaluate_validation(scores, loss_function, labels.cuda())\n",
    "            loss_sum += loss_batch\n",
    "            n_correct += n_corr_batch\n",
    "            n_batches += 1\n",
    "        val_acc = n_correct / n_valid\n",
    "        val_loss = loss_sum / n_batches\n",
    "\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)        \n",
    "\n",
    "        t1 = time.time()\n",
    "\n",
    "        if (i+1) % 10 == 0:\n",
    "\n",
    "            print(f'Epoch {i+1}: train loss = {train_loss:.4f}, val loss = {val_loss:.4f}, val acc: {val_acc:.4f}, time = {t1-t0:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: train loss = 7.3366, val loss = 7.8378, val acc: 0.0843, time = 5.5491\n",
      "Epoch 20: train loss = 6.8520, val loss = 7.3830, val acc: 0.0866, time = 5.4632\n",
      "Epoch 0  Top-1 Accuracy, No Filter: 0.08177083333333333\n",
      "Epoch 0  Top-2 Accuracy, No Filter: 0.16354166666666667\n",
      "Epoch 0  Top-1 Accuracy, With Filter: 0.6447916666666667\n",
      "Epoch 0  Top-2 Accuracy, With Filter: 0.8442708333333333\n",
      "Epoch 10: train loss = 6.6207, val loss = 7.2157, val acc: 0.0853, time = 5.4295\n",
      "Epoch 20: train loss = 6.4575, val loss = 7.1387, val acc: 0.0897, time = 5.4296\n",
      "Epoch 20  Top-1 Accuracy, No Filter: 0.08489583333333334\n",
      "Epoch 20  Top-2 Accuracy, No Filter: 0.16979166666666667\n",
      "Epoch 20  Top-1 Accuracy, With Filter: 0.6984375\n",
      "Epoch 20  Top-2 Accuracy, With Filter: 0.91875\n",
      "Epoch 10: train loss = 6.3587, val loss = 6.9904, val acc: 0.0868, time = 5.3490\n",
      "Epoch 20: train loss = 6.2818, val loss = 6.9272, val acc: 0.0896, time = 5.3845\n",
      "Epoch 40  Top-1 Accuracy, No Filter: 0.08802083333333334\n",
      "Epoch 40  Top-2 Accuracy, No Filter: 0.17604166666666668\n",
      "Epoch 40  Top-1 Accuracy, With Filter: 0.7171875\n",
      "Epoch 40  Top-2 Accuracy, With Filter: 0.9333333333333333\n",
      "Epoch 10: train loss = 6.2048, val loss = 6.8987, val acc: 0.0891, time = 5.4065\n",
      "Epoch 20: train loss = 6.1826, val loss = 6.8547, val acc: 0.0888, time = 5.4939\n",
      "Epoch 60  Top-1 Accuracy, No Filter: 0.0921875\n",
      "Epoch 60  Top-2 Accuracy, No Filter: 0.184375\n",
      "Epoch 60  Top-1 Accuracy, With Filter: 0.7015625\n",
      "Epoch 60  Top-2 Accuracy, With Filter: 0.9057291666666667\n",
      "Epoch 10: train loss = 6.1211, val loss = 6.8000, val acc: 0.0855, time = 5.4279\n",
      "Epoch 20: train loss = 6.0711, val loss = 6.7423, val acc: 0.0882, time = 5.3617\n",
      "Epoch 80  Top-1 Accuracy, No Filter: 0.1\n",
      "Epoch 80  Top-2 Accuracy, No Filter: 0.2\n",
      "Epoch 80  Top-1 Accuracy, With Filter: 0.7244791666666667\n",
      "Epoch 80  Top-2 Accuracy, With Filter: 0.9463541666666667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs_per_step = 20\n",
    "total_epochs = 0\n",
    "\n",
    "for i in range(5):\n",
    "    valid_seen = 0\n",
    "    n_correct = 0\n",
    "    n_correct_s = 0\n",
    "    n_correct_2 = 0\n",
    "    n_correct_2_s = 0\n",
    "    train_model(model, epochs_per_step)\n",
    "    model.eval()\n",
    "    for b, batch in enumerate(dev_iter):\n",
    "            label_idxs = [random.randint(0,11) for seq in batch]\n",
    "            labels = torch.cat(tuple([batch[i][label_idxs[i]:label_idxs[i]+1] for i in range(len(label_idxs))]))\n",
    "\n",
    "            sims = [get_similar_words(tokenizer.decode([label])) for label in labels]\n",
    "            labels_sound_like = [tokenizer.encode(\" \".join(sim)) for sim in sims]\n",
    "\n",
    "\n",
    "            for j in range(len(label_idxs)):\n",
    "                batch[j] = torch.cat((batch[j][:label_idxs[j]], batch[j][label_idxs[j]+1:], batch[j][label_idxs[j]:label_idxs[j]+1]))\n",
    "            scores = model(batch.transpose(1,0).cuda())\n",
    "            n_corr_batch, n_corr_batch_2, _ = evaluate_validation(scores, loss_function, labels.cuda())\n",
    "            n_correct += n_corr_batch\n",
    "            n_correct_2 += n_corr_batch_2\n",
    "            n_corr_batch_s, n_corr_batch_2_s = eval_with_sound(scores, labels_sound_like, labels.cuda())\n",
    "            n_correct_s += n_corr_batch_s\n",
    "            n_correct_2_s += n_corr_batch_2_s\n",
    "            if b% 10 == 0:\n",
    "                pass\n",
    "            valid_seen = b * 64\n",
    "            if b >=30:\n",
    "                break\n",
    "    val_acc = n_correct / valid_seen\n",
    "    print(\"Epoch\", (i+1)*epochs_per_step, \" Top-1 Accuracy, No Filter:\", val_acc)\n",
    "    val_acc_2 = n_correct_2 / valid_seen\n",
    "    print(\"Epoch\", (i+1)*epochs_per_step, \" Top-2 Accuracy, No Filter:\", val_acc_2)\n",
    "    val_acc_s = n_correct_s / valid_seen\n",
    "    print(\"Epoch\", (i+1)*epochs_per_step, \" Top-1 Accuracy, With Filter:\", val_acc_s)\n",
    "    val_acc_2_s = n_correct_2_s / valid_seen\n",
    "    print(\"Epoch\", (i+1)i*epochs_per_step, \" Top-2 Accuracy, With Filter:\", val_acc_2_s)\n",
    "0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:NLP]",
   "language": "python",
   "name": "conda-env-NLP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
